<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Zach Stagers</title>
    <link>https://www.zachstagers.co.uk/post/</link>
    <description>Recent content in Posts on Zach Stagers</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Mar 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://www.zachstagers.co.uk/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>What Makes a Good Lakehouse Serving Layer?</title>
      <link>https://www.zachstagers.co.uk/p/what-makes-a-good-lakehouse-serving-layer/</link>
      <pubDate>Wed, 30 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.zachstagers.co.uk/p/what-makes-a-good-lakehouse-serving-layer/</guid>
      <description>This post is a bit of an exploratory one, collecting some thoughts about features which combine to make a good data lakehouse serving layer, and the things to consider when choosing a resource to serve your data.
I do not intend for this post to act as a guide for choosing the right serving layer, but instead it&amp;rsquo;s an introduction to the subject and some things to watch out for as you identify which tool would be best for you.</description>
    </item>
    
    <item>
      <title>Azure Synapse Serverless Lake Access Patterns</title>
      <link>https://www.zachstagers.co.uk/p/azure-synapse-serverless-lake-access-patterns/</link>
      <pubDate>Wed, 23 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.zachstagers.co.uk/p/azure-synapse-serverless-lake-access-patterns/</guid>
      <description>I&amp;rsquo;ve been working with a client to build a large Data Lakehouse platform predominantly using Databricks as a processing engine and Synapse Serverless to serve the data out in a couple different ways.
This was the first time I&amp;rsquo;d properly used Synapse Serverless in anger, and to say I found the documentation around lake access confusing would be an understatement! Everything I needed to understand was spread over a few different articles, which I&amp;rsquo;ve linked to at the bottom of this post.</description>
    </item>
    
    <item>
      <title>SQLBits 2022</title>
      <link>https://www.zachstagers.co.uk/p/sqlbits-2022/</link>
      <pubDate>Wed, 02 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.zachstagers.co.uk/p/sqlbits-2022/</guid>
      <description>This years arcade-themed SQLBits is taking place between March 8th and 12th. I&amp;rsquo;ve been to SQLBits a number of times, it&amp;rsquo;s always great fun and I come away having learned a ton!
However, this year is a little different from me, as I&amp;rsquo;ll be presenting for the first time! I&amp;rsquo;m looking forward to it a lot, the last time I presented at a community event was on Azure Notebooks back in July 2018 (as seen in the banner image)!</description>
    </item>
    
    <item>
      <title>Changing a Data Factory Integration Runtime Registration Key</title>
      <link>https://www.zachstagers.co.uk/p/changing-a-data-factory-integration-runtime-registration-key/</link>
      <pubDate>Tue, 14 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.zachstagers.co.uk/p/changing-a-data-factory-integration-runtime-registration-key/</guid>
      <description>Introduction When you&amp;rsquo;ve installed an integration runtime (IR) and connected it to an Azure Data Factory (ADF), it isn&amp;rsquo;t exactly obvious how you change which Data Factory that IR is connected to. This short blog talks you through the process of updating an IR&amp;rsquo;s connection.
Steps to change the IR&amp;rsquo;s Key Firstly, confirm the IR is in fact already registered to a Data Factory. Open the Integration Runtime Configuration Manager and you should see a green tick and &amp;lsquo;Self-hosted node is connected to the cloud service&amp;rsquo; with details of the ADF resource it&amp;rsquo;s connected to.</description>
    </item>
    
    <item>
      <title>Connect Azure Databricks to a DevOps repo in a different tenancy</title>
      <link>https://www.zachstagers.co.uk/p/connect-azure-databricks-to-a-devops-repo-in-a-different-tenancy/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.zachstagers.co.uk/p/connect-azure-databricks-to-a-devops-repo-in-a-different-tenancy/</guid>
      <description>If you&amp;rsquo;re working with a large multi-tenancy organisation it&amp;rsquo;s possible the subscription your Databricks resource sits in is a different tenancy to the Azure DevOps hosting your repositories. This blog explains how to connect Databricks to a DevOps repository in that scenario.
When trying to connect to DevOps in a seperate tenancy, you&amp;rsquo;ll receive the message Unable to parse credentials from Azure Active Directory account. Ensure Azure Devops account is connected to AAD.</description>
    </item>
    
    <item>
      <title>Azure Data Lake ACL Introduction</title>
      <link>https://www.zachstagers.co.uk/p/azure-data-lake-acl-introduction/</link>
      <pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.zachstagers.co.uk/p/azure-data-lake-acl-introduction/</guid>
      <description>Introduction to ACL&amp;rsquo;s Access Control Lists (ACLs) offer low-level control of access to the folders within your Azure Data Lake, whilst Role-Based Access Control (RBAC) offers high-level control to the entire lake.
RBAC vs ACL
The ACL permissions on offer are:
 Read - Grants read access to files and folders (i.e. able to see the contents). Write - Grants write access to files and folders. Execute - Grants&amp;hellip; execute.</description>
    </item>
    
  </channel>
</rss>
