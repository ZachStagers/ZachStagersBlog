<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Zach Stagers</title>
        <link>https://www.zachstagers.co.uk/</link>
        <description>Recent content on Zach Stagers</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 03 Jul 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://www.zachstagers.co.uk/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Data and AI Summit 2022 Announcements</title>
        <link>https://www.zachstagers.co.uk/p/data-and-ai-summit-2022-announcements/</link>
        <pubDate>Sun, 03 Jul 2022 00:00:00 +0000</pubDate>
        
        <guid>https://www.zachstagers.co.uk/p/data-and-ai-summit-2022-announcements/</guid>
        <description>&lt;img src="https://www.zachstagers.co.uk/p/data-and-ai-summit-2022-announcements/banner_data_and_ai_summit_2022_announcements.jpg" alt="Featured image of post Data and AI Summit 2022 Announcements" /&gt;&lt;h3 id=&#34;data-and-ai-summit-2022&#34;&gt;Data and AI Summit 2022&lt;/h3&gt;
&lt;p&gt;I&amp;rsquo;m absolutely buzzing after attending my first Data and AI Summit (DAAIS) in San Francisco last week (June 27th to 30th). Incredible key notes, fantastic venue and city, good people, and awesome after parties! Huge thank you to everyone at Databricks for putting the event together, and to Advancing Analytics for giving me the opportunity to attend.&lt;/p&gt;
&lt;p&gt;As I sit on my flight home, mind racing over the events of the week, I wanted to be productive and write up some of the key announcements and share some initial thoughts. We&amp;rsquo;ll definitely be diving deeper into each of these with more targeted posts &amp;amp; videos over on our YouTube, and as we do that, I&amp;rsquo;ll update this post with the relevant links.&lt;/p&gt;
&lt;p&gt;Anyway, here&amp;rsquo;s a quick lowdown of some of the announcements in no particular order&amp;hellip; Let me know what you&amp;rsquo;re most excited to get a closer look at.&lt;/p&gt;
&lt;h3 id=&#34;delta-20&#34;&gt;Delta 2.0&lt;/h3&gt;
&lt;p&gt;One of the biggest cheers of the keynote was that Delta is being fully open sourced! Databricks continue to share their incredible work to help drive our industry forward. Delta already has wide adoption, but with the open sourced version now being levelled up to the same standard as the &amp;lsquo;proprietary&amp;rsquo; one, this should help cement it as the default choice for lake-based storage.&lt;/p&gt;
&lt;p&gt;There were some announcements of things to come with Delta too, such as a optimised deletes and updates by removing single rows instead of having to completely rewrite the file. It&amp;rsquo;ll be really interesting to see how this works, and just how much it boosts performance.&lt;/p&gt;
&lt;p&gt;&lt;figure style=&#34;flex-grow: 182; flex-basis: 439px&#34;&gt;
		&lt;a href=&#34;https://www.zachstagers.co.uk/p/data-and-ai-summit-2022-announcements/delta.JPG&#34; data-size=&#34;900x492&#34;&gt;&lt;img src=&#34;https://www.zachstagers.co.uk/p/data-and-ai-summit-2022-announcements/delta.JPG&#34;
				
				width=&#34;900&#34;
				height=&#34;492&#34;
				loading=&#34;lazy&#34;
				alt=&#34;Delta Announcements.&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;Delta Announcements.&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;h3 id=&#34;delta-sharing-and-cleanrooms&#34;&gt;Delta Sharing and Cleanrooms&lt;/h3&gt;
&lt;p&gt;Delta Sharing is going Generally Available (GA), and has had more connectors added this year (such as PowerBI), and has more connectors coming soon (such as Terraform, Tableau, and Airflow).&lt;/p&gt;
&lt;p&gt;On top of that, Cleanrooms was announced and has really caught my attention. It&amp;rsquo;s part of Delta Sharing and provides a way of tightly controlling access to your shared data sets. It looks to include things like external query approval, which means you&amp;rsquo;ll have total visibility of the data and aggregations people are trying (if approved) to view.&lt;/p&gt;
&lt;p&gt;This feels like it could be huge in industries like healthcare, finance, and other heavily regulated industries. I expect this&amp;rsquo;ll unlock the ability for those with sensitive data to more easily work with third parties for data analysis and science, whilst never exposing PII type field, for example.&lt;/p&gt;
&lt;p&gt;&lt;figure style=&#34;flex-grow: 182; flex-basis: 437px&#34;&gt;
		&lt;a href=&#34;https://www.zachstagers.co.uk/p/data-and-ai-summit-2022-announcements/cleanrooms.JPG&#34; data-size=&#34;902x495&#34;&gt;&lt;img src=&#34;https://www.zachstagers.co.uk/p/data-and-ai-summit-2022-announcements/cleanrooms.JPG&#34;
				
				width=&#34;902&#34;
				height=&#34;495&#34;
				loading=&#34;lazy&#34;
				alt=&#34;Cleanrooms Announcement.&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;Cleanrooms Announcement.&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;h3 id=&#34;databricks-marketplace&#34;&gt;Databricks Marketplace&lt;/h3&gt;
&lt;p&gt;More than just a data asset marketplace for things like weather data, but also for ML models, solution accelerators like Hydr8, notebooks, and dashboards. There wasn&amp;rsquo;t much detail given around actually listing something in the marketplace or charging options, so it&amp;rsquo;ll be interesting to see how this all works. Could be handy.&lt;/p&gt;
&lt;h3 id=&#34;spark-connect&#34;&gt;Spark Connect&lt;/h3&gt;
&lt;p&gt;A new API for submitting Spark jobs. Will most likely open up better integration for IDE&amp;rsquo;s such as VSCode, but also means other lightweight applications will have a way of submitting Spark jobs. Perhaps developing notebooks from your watch is just around the corner?&lt;/p&gt;
&lt;p&gt;&lt;figure style=&#34;flex-grow: 182; flex-basis: 438px&#34;&gt;
		&lt;a href=&#34;https://www.zachstagers.co.uk/p/data-and-ai-summit-2022-announcements/spark_connect.JPG&#34; data-size=&#34;900x493&#34;&gt;&lt;img src=&#34;https://www.zachstagers.co.uk/p/data-and-ai-summit-2022-announcements/spark_connect.JPG&#34;
				
				width=&#34;900&#34;
				height=&#34;493&#34;
				loading=&#34;lazy&#34;
				alt=&#34;Spark Connect Announcement.&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;Spark Connect Announcement.&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;h3 id=&#34;unity-catalog&#34;&gt;Unity Catalog&lt;/h3&gt;
&lt;p&gt;This has been much anticipated. Matei Zaharia himself came out on stage to demo this one, with CEO Ali Ghodsi saying they&amp;rsquo;d put the &amp;lsquo;smartest man in the company&amp;rsquo; on the project!&lt;/p&gt;
&lt;p&gt;Matei&amp;rsquo;s demo did a great job of showing off some of the lineage capabilities, with both up &amp;amp; down stream lineage being available. Upstream being &amp;ldquo;Where this entity was sourced from&amp;rdquo; and downstream being &amp;ldquo;Where this entity is fed into&amp;rdquo;. Really handy for measuring impact of editting a particular notebook and building test plans for example.&lt;/p&gt;
&lt;p&gt;Simon Whiteley&amp;rsquo;s already had a dig into this one and published a video, which you can see here; &lt;a class=&#34;link&#34; href=&#34;youtu.be/FCuuFGS3jFM&#34; &gt;Advancing Analytics YouTube; Unity Catalog Intro&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Unity Catalog will be going GA in the coming weeks!&lt;/p&gt;
&lt;h3 id=&#34;databricks-sql-serverless&#34;&gt;Databricks SQL Serverless&lt;/h3&gt;
&lt;p&gt;SQL Serverless has been in private preview for a little while, but it&amp;rsquo;s going public, although unfortunately only on AWS for now. We got some great sight of what it can do though, and a good look at their Serverless roadmap. Some of the key takeaways on SQL Serverless were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There&amp;rsquo;s still a cluster, so it&amp;rsquo;s a little different to Synapse Serverless which is exciting.&lt;/li&gt;
&lt;li&gt;Seeing start up times of around 10 seconds for initial query, but hope to have this as low as 2 seconds by the time it goes GA. 10 seconds may seem high, but remember right now it can take 5 minutes for a cluster to start up.&lt;/li&gt;
&lt;li&gt;They are seeing customers achieve a 20-40% reduction in cost by eradicating wasted cluster uptime!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can see the roadmap here on my Twitter; &lt;a class=&#34;link&#34; href=&#34;https://twitter.com/ZachStagers/status/1541861734968832000&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Serverless Roadmap&lt;/a&gt;. Serverless notebooks later this year by the looks of things!&lt;/p&gt;
&lt;h3 id=&#34;project-lightspeed-streaming-improvements&#34;&gt;Project Lightspeed (Streaming Improvements)&lt;/h3&gt;
&lt;p&gt;Databricks themselves have already published a ton of info on this, which you can find here;&lt;/p&gt;
&lt;p&gt;The highlight for me is that they are talking about a 4x improvement in processing latency!&lt;/p&gt;
&lt;p&gt;&lt;figure style=&#34;flex-grow: 181; flex-basis: 436px&#34;&gt;
		&lt;a href=&#34;https://www.zachstagers.co.uk/p/data-and-ai-summit-2022-announcements/lightspeed.JPG&#34; data-size=&#34;902x496&#34;&gt;&lt;img src=&#34;https://www.zachstagers.co.uk/p/data-and-ai-summit-2022-announcements/lightspeed.JPG&#34;
				
				width=&#34;902&#34;
				height=&#34;496&#34;
				loading=&#34;lazy&#34;
				alt=&#34;Project Lightspeed Announcement.&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;Project Lightspeed Announcement.&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;h3 id=&#34;enzyme&#34;&gt;Enzyme&lt;/h3&gt;
&lt;p&gt;A little bit cloak and dagger this one… Not entirely sure where it fits or what it really is, but it looks to be an ETL optimization project for Delta Live Tables (DLT) offering data ingestion/acquisition capabilities perhaps, a lot of talk about it’s ability to automatically choose the best method for incremental ingestion (or incrementalization as Michael Armbrust called it), such as append only, partition recompiling, or merging.&lt;/p&gt;
&lt;p&gt;Sounds like a broad optimization to DLT, but if it does offer any acquisition capabilities then I’d love to see it as part of Workflows (below) too. Time will tell…&lt;/p&gt;
&lt;h3 id=&#34;new-sql-rest-api&#34;&gt;New SQL REST API&lt;/h3&gt;
&lt;p&gt;A new API targeted at Databricks SQL so you can launch queries at it from other services. Could enable things like embedding live visuals into web-apps backed by Databricks SQL.&lt;/p&gt;
&lt;h3 id=&#34;photon-improvements&#34;&gt;Photon Improvements&lt;/h3&gt;
&lt;p&gt;Firstly, it&amp;rsquo;s going Generally Available (GA), but they&amp;rsquo;ve made a bunch of improvements with it too.&lt;/p&gt;
&lt;p&gt;Some cool enhancements with Photon with it going GA, such as faster query plan generation. The example given during the key note given showed plan generation taking 1.8 seconds before the improvements and being optimised down to 0.7 seconds - a very nice 60% improvement! In both cases the query took 0.3 seconds to execute, so great to see such a reduction in the plan generation, where the majority of time is being spent.&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s also something in the works called Metadata Pipelinging to further improve performance, and early tests have shown an additional 0.3 second improvement on the query above, which would mean a combined improvement of a whopping 83%!&lt;/p&gt;
&lt;p&gt;Window Functions such as ROW_NUMBER and data sorting have both been optimised for improved performance. These will be in preview soon.&lt;/p&gt;
&lt;h3 id=&#34;databricks-workflows&#34;&gt;Databricks Workflows&lt;/h3&gt;
&lt;p&gt;A new method for orchestrating data platform operations within Databricks. It looks like a good start, but I think it has a little way to go before I&amp;rsquo;ll be picking it up over Data Factory.&lt;/p&gt;
&lt;p&gt;For example it doesn&amp;rsquo;t look to have iterators yet (for each, while), the dependencies look a little basic as there&amp;rsquo;s no on-success/failure yet, but I believe all of this and more is on the roadmap. It does have parameters, but these weren&amp;rsquo;t included in the demo.&lt;/p&gt;
&lt;p&gt;The default logging and monitoring looked great - super visual with some high level stats like late running pipelines.&lt;/p&gt;
&lt;p&gt;If Enzyme (above) does turn out to be an acquisition engine, it would be lovely if it were integrated here&amp;hellip;&lt;/p&gt;
&lt;p&gt;Keep an eye on this, I think this could be big too.&lt;/p&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;That&amp;rsquo;s a 37,000 feet view of some of the data engineering focused announcements quite literally written whilst at 37,000 feet. There&amp;rsquo;s a ton of exciting stuff to dig into here, and I&amp;rsquo;m hyped to be getting stuck into it all - stay tuned for some deeper content on all of the above.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Lakehouse Serving Options - 2022</title>
        <link>https://www.zachstagers.co.uk/p/lakehouse-serving-options-2022/</link>
        <pubDate>Thu, 19 May 2022 00:00:00 +0000</pubDate>
        
        <guid>https://www.zachstagers.co.uk/p/lakehouse-serving-options-2022/</guid>
        <description>&lt;img src="https://www.zachstagers.co.uk/p/lakehouse-serving-options-2022/banner_lakehouse_serving_options_2022.jpg" alt="Featured image of post Lakehouse Serving Options - 2022" /&gt;&lt;p&gt;I blogged previously on my thoughts about &lt;a class=&#34;link&#34; href=&#34;https://www.zachstagers.co.uk/p/what-makes-a-good-lakehouse-serving-layer/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;what makes a good lakehouse serving layer&lt;/a&gt; - the functionality and features to consider when trying to identify the right serving tool for you.&lt;/p&gt;
&lt;p&gt;In this post I want to go a step further and do a direct comparison of some of the more obvious Azure based lakehouse serving tools - Synapse Serverless, Databricks SQL, and Synapse Dedicated. I&amp;rsquo;ve decided to include Dedicated here, as although it defeats the purpose of the lakehouse paradigm, I&amp;rsquo;m still often asked whether it&amp;rsquo;s suitable.&lt;/p&gt;
&lt;p&gt;You&amp;rsquo;ll notice I&amp;rsquo;ve put 2022 in the title of this blog. This is the type of thing that&amp;rsquo;ll have a changing landscape as these tools develop further, and new tools emerge, so I wanted to create that distinction now to support future editions of this post.&lt;/p&gt;
&lt;h3 id=&#34;functionality-comparison&#34;&gt;Functionality Comparison&lt;/h3&gt;
&lt;p&gt;In my previous post, I called out 5 main things to look out for when choosing a tool; Concurrency, Security, Performance, Integration, and Cost. Below I&amp;rsquo;ve produced a comparison chart across these 5 metrics for the tools in scope for this blog.&lt;/p&gt;
&lt;p&gt;&lt;figure style=&#34;flex-grow: 179; flex-basis: 431px&#34;&gt;
		&lt;a href=&#34;https://www.zachstagers.co.uk/p/lakehouse-serving-options-2022/lakehouse_serving_options_2022_zach_stagers.JPG&#34; data-size=&#34;3083x1715&#34;&gt;&lt;img src=&#34;https://www.zachstagers.co.uk/p/lakehouse-serving-options-2022/lakehouse_serving_options_2022_zach_stagers.JPG&#34;
				
				width=&#34;3083&#34;
				height=&#34;1715&#34;
				loading=&#34;lazy&#34;
				alt=&#34;2022 Lakehouse Serving Tool Comparison Chart.&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;2022 Lakehouse Serving Tool Comparison Chart.&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;For reference, here&amp;rsquo;s a link to &lt;a class=&#34;link&#34; href=&#34;https://www.vldb.org/pvldb/vol13/p3204-saborit.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Synapse Serverless&#39; Polaris Engine White Paper&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The comparison chart above condenses a lot of information into a single table, but of course there are other things to consider. Things like the maintenance involved with managing each of those tools, how arduous the set up and configuration is, and their unique abilities (or USP&amp;rsquo;s).&lt;/p&gt;
&lt;h3 id=&#34;setup-and-configuration&#34;&gt;Setup and Configuration&lt;/h3&gt;
&lt;p&gt;By setup and configuration, I mean all of the work involved with getting to the point you can actually serve data from the tool - excluding your specific security setup&amp;hellip;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Synapse Dedicated&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In order to serve out of a dedicated SQL pool, you would first need to ingest the data into that pool. This could potentially include creating credentials, external file formats, external data source, external tables, and using PolyBase, or using the COPY command.&lt;/p&gt;
&lt;p&gt;Once the data is into a dedicated pool, it may need merging from staging tables into serving tables using slowly changing techniques (SCD).&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s quite a lot of configuration, plus you have to pay an additional ~£18 per TB of data stored inside the dedicated pool each month.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Synapse Serverless&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Similar to Dedicated, but a little simpler. Here we need to provide a method of reading from the lake which may again include creating credentials, external file formats, and external data sources - plus views over those entities from which to serve, but there&amp;rsquo;s no PolyBase or COPY or data storage.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve blogged previously about &lt;a class=&#34;link&#34; href=&#34;https://www.zachstagers.co.uk/p/azure-synapse-serverless-lake-access-patterns/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Azure Synapse Serverless Lake Access Patterns&lt;/a&gt;, so I won&amp;rsquo;t go into any more detail on that here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Databricks SQL&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Perhaps the easiest of the lot. Assuming that you&amp;rsquo;re using Databricks as your processing engine, you probably already have your tables saved within the Hive metastore, which makes them available for querying in Databricks SQL.&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;re not using Databricks for your processing, then you&amp;rsquo;d need to mount your lake, create a database, and create your tables specifying the lake location to be read from.&lt;/p&gt;
&lt;h3 id=&#34;maintenance&#34;&gt;Maintenance&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Synapse Dedicated&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Managing a Dedicated pool can be quite involved. You need to think about the distribution method used for each table to avoid skew, whether table replication is appropriate, and the more traditional maintenance that comes with indexing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Synapse Serverless&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Aside from ensuring your lake is in order, there really isn&amp;rsquo;t any maintenance involved with Serverless - that&amp;rsquo;s the beauty of Polaris. If you&amp;rsquo;re using the Delta format (you should be) then you&amp;rsquo;ll want to apply vacuuming and optimising occasionally to keep things tip top.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Databricks SQL&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;With Databricks SQL you have an endpoint/clusters to monitor and manage, which comes with some overhead. On top of that, you should again ensure your Delta files are in good shape by running a vacuum and optimise every now and then.&lt;/p&gt;
&lt;h3 id=&#34;unique-selling-points&#34;&gt;Unique Selling Points&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Synapse Dedicated&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It doesn&amp;rsquo;t really have any unique serving capabilities when compared to Synapse Serverless and Databricks SQL.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Synapse Serverless&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The beauty of Serverless is the Polaris engine. There&amp;rsquo;s absolutely nothing to manage - no cluster to spin up, or sizing. On the other hand, this means it&amp;rsquo;s a little black boxy, but there&amp;rsquo;s plenty of guidance out there about how to optimise serverless queries (hint: use Delta and specify a WITH clause with narrow data types!).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Databricks SQL&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Databricks SQL offers the in-built ability to create visualisations, dashboards, and alerts. Meaning, if it caters to all your visualisation needs, and you go all in on Databricks, you may not even need PowerBI licencing! Suddenly the price tag for a Databricks SQL cluster really doesn&amp;rsquo;t look too bad&amp;hellip;&lt;/p&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Hopefully this post has helped you decide on how you want to serve your lake. On the surface it may appear that Serverless is the outright winner here, but Databricks SQL is a fantastic option too. If you&amp;rsquo;re only using Synapse for one of it&amp;rsquo;s many capabilities (in this case Serverless), the fact you could eliminate it from your architecture all together and therefore not have another resource/workspace to manage is a really big deal.&lt;/p&gt;
&lt;p&gt;It feels as though I&amp;rsquo;ve been pretty harsh on ol&#39; Synapse Dedicated SQL Pools today&amp;hellip; but in the age of lakes, it is not king, and I hope to have painted a clear and obvious picture of the reasons why it isn&amp;rsquo;t suitable for serving a lake-based analytical model.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>What Makes a Good Lakehouse Serving Layer?</title>
        <link>https://www.zachstagers.co.uk/p/what-makes-a-good-lakehouse-serving-layer/</link>
        <pubDate>Wed, 30 Mar 2022 00:00:00 +0000</pubDate>
        
        <guid>https://www.zachstagers.co.uk/p/what-makes-a-good-lakehouse-serving-layer/</guid>
        <description>&lt;img src="https://www.zachstagers.co.uk/p/what-makes-a-good-lakehouse-serving-layer/banner_what_makes_a_good_lakehouse_serving_layer.jpg" alt="Featured image of post What Makes a Good Lakehouse Serving Layer?" /&gt;&lt;p&gt;This post is a bit of an exploratory one, collecting some thoughts about features which combine to make a good data lakehouse serving layer, and the things to consider when choosing a resource to serve your data.&lt;/p&gt;
&lt;p&gt;I do not intend for this post to act as a guide for choosing the right serving layer, but instead it&amp;rsquo;s an introduction to the subject and some things to watch out for as you identify which tool would be best for you.&lt;/p&gt;
&lt;h3 id=&#34;what-is-a-serving-layer&#34;&gt;What is a serving layer?&lt;/h3&gt;
&lt;p&gt;Simply put, a serving layer is a method of presenting data to users. Or put differently, it&amp;rsquo;s a tool through which users can access data.&lt;/p&gt;
&lt;p&gt;But when you break that simple statement down, there&amp;rsquo;s a lot to think about! What data is being presented back, and who is it being presented to? How do users want to connect to the serving layer (assuming they have a choice)? And when they&amp;rsquo;ve connected, how do we ensure they only see the data they&amp;rsquo;re supposed to see? How much data is there? How many users are there? What timezones do they operate in?&lt;/p&gt;
&lt;p&gt;Taking the answers to these questions, we can build a picture of our requirements, and therefore the features we need to think about and therefore what makes a serving layer good.&lt;/p&gt;
&lt;h3 id=&#34;why-do-you-need-a-serving-layer&#34;&gt;Why do you need a serving layer?&lt;/h3&gt;
&lt;p&gt;A big selling point of the data lakehouse is the lakes ability to service a broad range of users, directly from the lake. But  all the lake does is store data - it has no compute ability with which to query that data.&lt;/p&gt;
&lt;p&gt;Regardless of whether you have analysts exploring the lake or you have a centrally-controlled and developed enterprise model, you need a compute resource to query that lake data.&lt;/p&gt;
&lt;h3 id=&#34;what-features-do-we-look-for-in-a-serving-area&#34;&gt;What features do we look for in a serving area?&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;1. Concurrency&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If we know we have 100&amp;rsquo;s or even 1,000&amp;rsquo;s of users who&amp;rsquo;ll be querying our data model every day, it&amp;rsquo;s no use copying our data into a Synapse Dedicated pool. Not only would that be an expensive option, but dedicated pool&amp;rsquo;s maximum concurrency limit of 128 would quickly cause delays and performance issues. It would also defeat the purpose of the lakehouse paradigm by duplicating your data to somewhere outside of the lake.&lt;/p&gt;
&lt;p&gt;We need to ensure the tool can handle the number of users and queries, remembering that each visual on a dashboard equates to a query, this can add up fast.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Security&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Row Level Security (RLS) has long been method for enhancing data security at the serving layer, and is often an important feature to look out for in the majority of projects.&lt;/p&gt;
&lt;p&gt;Other things to consider here is how well it integrates with your identity management system, whether that be Azure Active Directory (AAD) or something like Okta.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. Performance&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Performance is one of those things that can go completely unnoticed when something is fast, but can be the bane of everyone&amp;rsquo;s life when it&amp;rsquo;s slow!&lt;/p&gt;
&lt;p&gt;Some things to consider in this area are whether the first users to query each day will have to wait for a resource or cluster to start, or in a serverless scenario, how long does it generally take for a query to even &lt;em&gt;begin&lt;/em&gt; running.&lt;/p&gt;
&lt;p&gt;On the other hand, if we&amp;rsquo;re importing data into a PowerBI model, how long does that process take? This is especially important if we want to refresh our models throughout the day.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4. Integration&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s two sides to the serving layer to consider here. The integration between the lake and the serving layer, and the integration between the serving layer and user reporting tools like PowerBI.&lt;/p&gt;
&lt;p&gt;On the lake to serving layer integration, if we&amp;rsquo;re using a direct-query model, it absolutely must understand how to read a parquet file or delta table to be a suitable option for serving a data lakehouse. Without this, querying any datasets on the larger side would likely be unacceptably slow for users.&lt;/p&gt;
&lt;p&gt;Whereas on the other side, does it play nicely with the reporting tools users will be connecting with such as PowerBI, Tableau, Looker, &amp;hellip;or Excel.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5. Cost&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;OK, cost isn&amp;rsquo;t really a &lt;em&gt;feature&lt;/em&gt;, but it&amp;rsquo;s almost always one of the biggest factors in choosing any component of a data platform.&lt;/p&gt;
&lt;p&gt;Things to consider when you&amp;rsquo;re on a tight budget is whether the resource needs a cluster up and running, and if so what the uptime scheduling for that looks like and how big the cluster needs to be to support the number of users you&amp;rsquo;re expecting.&lt;/p&gt;
&lt;h3 id=&#34;what-options-are-there&#34;&gt;What options are there?&lt;/h3&gt;
&lt;p&gt;Gone are the days of Analysis Services&#39; dominance in the serving arena! There are a host of tools available which offer lake based serving capabilities.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Azure Synapse&lt;/li&gt;
&lt;li&gt;Databricks&lt;/li&gt;
&lt;li&gt;Azure Analysis Services&lt;/li&gt;
&lt;li&gt;PowerBI (Import Mode Only)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are obviously very Azure / Microsoft focused, which is the space I tend to operate within, but of course there are plenty of other vendors which can read and serve lake data too.&lt;/p&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;As I&amp;rsquo;ve written these thoughts and considerations out, it&amp;rsquo;s dawned on me just how complex and nuanced a topic this is!&lt;/p&gt;
&lt;p&gt;In a future post I&amp;rsquo;d like to explore the serving options further, their strengths and weaknesses in the areas I&amp;rsquo;ve mentioned in this post, and the circumstances under which each may be the most appropriate choice.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;d be interested to hear other peoples thoughts on this subject.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Azure Synapse Serverless Lake Access Patterns</title>
        <link>https://www.zachstagers.co.uk/p/azure-synapse-serverless-lake-access-patterns/</link>
        <pubDate>Wed, 23 Mar 2022 00:00:00 +0000</pubDate>
        
        <guid>https://www.zachstagers.co.uk/p/azure-synapse-serverless-lake-access-patterns/</guid>
        <description>&lt;img src="https://www.zachstagers.co.uk/p/azure-synapse-serverless-lake-access-patterns/banner_azure_synapse_serverless_lake_access_patterns.jpg" alt="Featured image of post Azure Synapse Serverless Lake Access Patterns" /&gt;&lt;p&gt;I&amp;rsquo;ve been working with a client to build a large Data Lakehouse platform predominantly using Databricks as a processing engine and Synapse Serverless to serve the data out in a couple different ways.&lt;/p&gt;
&lt;p&gt;This was the first time I&amp;rsquo;d properly used Synapse Serverless in anger, and to say I found the documentation around lake access confusing would be an understatement! Everything I needed to understand was spread over a few different articles, which I&amp;rsquo;ve linked to at the bottom of this post.&lt;/p&gt;
&lt;p&gt;The crux of it is that there&amp;rsquo;s several ways to configure access to your lake data from Synapse Serverless, and in this blog I&amp;rsquo;d like to step through the ways we&amp;rsquo;ve configured it, and what those configurations are good for. This post does not speak to network security, although I have provided a link to the Azure Synapse Network Security white paper at the end.&lt;/p&gt;
&lt;h3 id=&#34;active-directory-ad-passthrough-aka-user-identity&#34;&gt;Active Directory (AD) Passthrough (a.k.a User Identity)&lt;/h3&gt;
&lt;p&gt;This is the default method of accessing data. If you simply open up your Synapse workspace and query your lake data using OpenRowset queries without creating and specifying a data source, your credentials will be passed down to the data lake layer to be authenticated.&lt;/p&gt;
&lt;p&gt;This means that in order for authentication to succeed, you need to have configured either RBAC or ACL&amp;rsquo;s for your user account;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RBAC (Role Based Access Control) - Course grain access control. Whatever RBAC role is assigned will apply to all files and folders in the lake. The role applied to permit access via Serverless must be one of the &amp;lsquo;Storage Blob Data ***&amp;rsquo; roles.&lt;/li&gt;
&lt;li&gt;ACL (Access Control Lists) - Fine grain access control. Grants access to specific files and folders. For a deeper look at ACL&amp;rsquo;s, see a previous blog of mine here: &lt;a class=&#34;link&#34; href=&#34;https://www.zachstagers.co.uk/p/azure-data-lake-acl-introduction/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Azure Data Lake ACL Introduction&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This option is great in two different ways, depending on which security protocol you use:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For allowing analysts free reign into the lake (RBAC).&lt;/li&gt;
&lt;li&gt;To have meticulous control over what people have access to (ACL).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;synapse-managed-service-identity-msi---database-scoped&#34;&gt;Synapse Managed Service Identity (MSI) - Database Scoped&lt;/h3&gt;
&lt;p&gt;If you don&amp;rsquo;t want to configure ACL&amp;rsquo;s or RBAC for your users, you can grant access via the Synapse managed service identity (MSI) using a database scoped credential. Managed identities are essentially a credential that is created alongside the resource which can be treated like any other active directory account - they typically have the same name as the resource they belong to. The MSI will still need to be granted lake level access (i.e. via RBAC or ACL, per the above section).&lt;/p&gt;
&lt;p&gt;To make use of the MSI, some set up is required. As we&amp;rsquo;re creating a database scoped, you&amp;rsquo;ll need to create a serverless database, then from within that database you need to create a master key to protect our credential.&lt;/p&gt;
&lt;p&gt;Next we create a database scoped credential specifying that the credential is based on the managed service identity.&lt;/p&gt;
&lt;p&gt;The final step is to create an external data source. This specifies our lake location via the data lake storage end point URL, and the credential to use to connect to that end point (in this case this will be our MSI credential). The location specified is the root of where access will be granted, meaning users accessing via this data source will have access to everything beneath the location specified.&lt;/p&gt;
&lt;p&gt;Putting all of this together, we end up with this script to enable lake access:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CREATE DATABASE SomeDatabase

USE SomeDatabase

CREATE MASTER KEY ENCRYPTION BY PASSWORD=&#39;&amp;lt;SuperStrongPassw0rd&amp;gt;&#39;

CREATE DATABASE SCOPED CREDENTIAL ManagedIdentityCredential
WITH IDENTITY = &#39;MANAGED SERVICE IDENTITY&#39;

CREATE EXTERNAL DATA SOURCE DataLakeContainer
WITH (
      LOCATION = &#39;https://&amp;lt;storageAccount&amp;gt;.dfs.core.windows.net/container/&#39;,
      CREDENTIAL = ManagedIdentityCredential
     );

&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To use the data source, we adapt our queries to include a data source parameter in our OpenRowset. Notice now that the BULK parameter, which would by default include the entire lake path, now only needs the path down from the location specified in our data source.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT
    TOP 100 *
FROM
    OPENROWSET(
        BULK &#39;/folder/deltaTable&#39;,
        FORMAT = &#39;DELTA&#39;,
        DATA_SOURCE = &#39;DataLakeContainer&#39;
    ) AS [result]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This option is again good in multiple ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For allowing analysts free reign into the lake from a certain access point and below.&lt;/li&gt;
&lt;li&gt;For developing a standard set of views which expose data to PowerBI.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;wrap-up&#34;&gt;Wrap Up&lt;/h3&gt;
&lt;p&gt;Using both of these methods together gives us a powerful blend of control and access. We&amp;rsquo;re able to serve a standardised and trusted enterprise data model to PowerBI using the MSI method, and we&amp;rsquo;re able to control access for analysts to splash around in the lake building their own models and metrics.&lt;/p&gt;
&lt;p&gt;This isn&amp;rsquo;t an exhaustive list of access methods - you could also use shared access signatures or service principals.&lt;/p&gt;
&lt;p&gt;Check out some of the security documentation for yourself:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/on-demand-workspace-overview#security&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Serverless SQL pool in Azure Synapse Analytics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-storage-files-storage-access-control?tabs=user-identity&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Control storage account access for serverless SQL pool in Azure Synapse Analytics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand#storage-access&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Self-help for serverless SQL pool&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/MicrosoftDocs/azure-docs/blob/main/articles/synapse-analytics/guidance/security-white-paper-network-security.md&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Azure Synapse Analytics security white paper: Network Security&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>SQLBits 2022</title>
        <link>https://www.zachstagers.co.uk/p/sqlbits-2022/</link>
        <pubDate>Wed, 02 Feb 2022 00:00:00 +0000</pubDate>
        
        <guid>https://www.zachstagers.co.uk/p/sqlbits-2022/</guid>
        <description>&lt;img src="https://www.zachstagers.co.uk/p/sqlbits-2022/banner_sqlbits_2022.jpg" alt="Featured image of post SQLBits 2022" /&gt;&lt;p&gt;This years &lt;a class=&#34;link&#34; href=&#34;https://arcade.sqlbits.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;arcade-themed SQLBits&lt;/a&gt; is taking place between March 8th and 12th. I&amp;rsquo;ve been to SQLBits a number of times, it&amp;rsquo;s always great fun and I come away having learned a ton!&lt;/p&gt;
&lt;p&gt;However, this year is a little different from me, as I&amp;rsquo;ll be presenting for the first time! I&amp;rsquo;m looking forward to it a lot, the last time I presented at a community event was on Azure Notebooks back in July 2018 (as seen in the banner image)!&lt;/p&gt;
&lt;h3 id=&#34;pre-con-a-data-engineers-guide-to-synapse-analytics&#34;&gt;Pre-Con: A Data Engineers Guide to Synapse Analytics&lt;/h3&gt;
&lt;p&gt;Alongside &lt;a class=&#34;link&#34; href=&#34;https://twitter.com/MrSiWhiteley&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Simon Whiteley&lt;/a&gt; and &lt;a class=&#34;link&#34; href=&#34;https://twitter.com/SQLStijn&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Stijn Wynants&lt;/a&gt;, I&amp;rsquo;ll be presenting a full day of Synapse training on Tuesday the 8th of March!&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;ll be an action packed day covering:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fundamentals of a lake based data platform&lt;/li&gt;
&lt;li&gt;Patterns for ingesting data and orchestrating your data platform execution&lt;/li&gt;
&lt;li&gt;How SQL Serverless Pools work and patterns for optimising performance and cost&lt;/li&gt;
&lt;li&gt;Using Spark Pools to write dynamic workflows in Python&lt;/li&gt;
&lt;li&gt;Utilizing Data Explorer Pools for deep exploration of logs, time series and other fast-moving unstructured data sources&lt;/li&gt;
&lt;li&gt;Integrating Synapse directly with tools such as Azure Purview and CosmosDB&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can learn more from our &lt;a class=&#34;link&#34; href=&#34;https://sessionize.com/s/zach-stagers/a_data_engineers_guide_to_azure_syn/47796&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;sessionize page&lt;/a&gt;, or by watching our short promotional clip&lt;/p&gt;
&lt;div class=&#34;video-wrapper&#34;&gt;
    &lt;iframe loading=&#34;lazy&#34; 
            src=&#34;https://www.youtube.com/embed/z0yZDcoGu-E&#34; 
            allowfullscreen 
            title=&#34;YouTube Video&#34;
    &gt;
    &lt;/iframe&gt;
&lt;/div&gt;

&lt;h3 id=&#34;session-synapse-data-flows---will-citizen-etl-replace-the-data-engineer&#34;&gt;Session: Synapse Data Flows - Will Citizen ETL Replace the Data Engineer?&lt;/h3&gt;
&lt;p&gt;As well as the pre-con, I&amp;rsquo;ll be presenting my own much shorter 20-minute session on Synapse Data Flows on Saturday the 12th of March, taking a look at how they measure up when compared to bespoke-built data platform. What&amp;rsquo;s the pipeline reusibility like? How well can you clean and standardize your data? What are they good at, and what are they bad at?&lt;/p&gt;
&lt;p&gt;You can learn more from my &lt;a class=&#34;link&#34; href=&#34;https://sessionize.com/s/zach-stagers/synapse_data_flows_-_will_citizen_e/47797&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;sessionize page&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;other-sessions-to-look-out-for&#34;&gt;Other sessions to look out for&lt;/h3&gt;
&lt;p&gt;Advancing Analytics will be at SQLBits in force. All together, we&amp;rsquo;re presenting 3 full day pre-conference training days and 11 general sessions! Find out what we&amp;rsquo;ll be presenting about and when via the &lt;a class=&#34;link&#34; href=&#34;https://www.advancinganalytics.co.uk/blog/2022/2/2/advancing-analytics-at-sqlbits-2022&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Advancing Analytics blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The full conference agenda can be found &lt;a class=&#34;link&#34; href=&#34;https://arcade.sqlbits.com/sessions/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Changing a Data Factory Integration Runtime Registration Key</title>
        <link>https://www.zachstagers.co.uk/p/changing-a-data-factory-integration-runtime-registration-key/</link>
        <pubDate>Tue, 14 Dec 2021 00:00:00 +0000</pubDate>
        
        <guid>https://www.zachstagers.co.uk/p/changing-a-data-factory-integration-runtime-registration-key/</guid>
        <description>&lt;img src="https://www.zachstagers.co.uk/p/changing-a-data-factory-integration-runtime-registration-key/banner_change_adf_ir.JPG" alt="Featured image of post Changing a Data Factory Integration Runtime Registration Key" /&gt;&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;When you&amp;rsquo;ve installed an integration runtime (IR) and connected it to an Azure Data Factory (ADF), it isn&amp;rsquo;t exactly obvious how you change which Data Factory that IR is connected to. This short blog talks you through the process of updating an IR&amp;rsquo;s connection.&lt;/p&gt;
&lt;h3 id=&#34;steps-to-change-the-irs-key&#34;&gt;Steps to change the IR&amp;rsquo;s Key&lt;/h3&gt;
&lt;p&gt;Firstly, confirm the IR is in fact already registered to a Data Factory. Open the Integration Runtime Configuration Manager and you should see a green tick and &amp;lsquo;Self-hosted node is connected to the cloud service&amp;rsquo; with details of the ADF resource it&amp;rsquo;s connected to. If you don&amp;rsquo;t see this, and instead see a text box with &amp;lsquo;Register Integration Runtime (Self-hosted)&amp;rsquo; above it then your IR isn&amp;rsquo;t registered and you can skip all of the steps below and just paste the key into the registration box and hit go.&lt;/p&gt;
&lt;p&gt;On to how to change which ADF resource your IR is registered with&amp;hellip;&lt;/p&gt;
&lt;p&gt;When you install an IR, a PowerShell script is placed alongside the installation which can be used for this task. Note that your folder path may not be identical to mine, as your IR may not have been installed in the default location, or you may have a different IR version.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;From the machine hosting the IR, open the Windows PowerShell ISE as Administrator (right click, Run as admin&amp;hellip;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Within the ISE click File, Open, and navigating to: &lt;code&gt;C:\Program Files\Microsoft Integration Runtime\5.0\PowerShellScript&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Open the &lt;code&gt;RegisterIntegrationRuntime.ps1&lt;/code&gt; PowerShell script.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Populate the parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;$gatewayKey&lt;/strong&gt; is the registration key of the new ADF resource you want to register the IR with. Remove the $throw so it&amp;rsquo;s just a string of the key.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$IsRegisteringOnRemoteMachine&lt;/strong&gt; can be left as it&amp;rsquo;s default, false, as in step 1 we&amp;rsquo;re running this script from the machine hosting the IR.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$NodeName&lt;/strong&gt; is the name that&amp;rsquo;ll appear in ADF when it&amp;rsquo;s registered. This is typically populated with the machine name hosting the IR.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Your script should look something like the screenshot below. Press run, it may take a minute or so, but once successful you&amp;rsquo;ll see the message &amp;lsquo;Integration Runtime registration is successful!&amp;rsquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Re-open the Integration Runtime Configuration Manager and confirm it is now registered with the new IR.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;figure style=&#34;flex-grow: 464; flex-basis: 1115px&#34;&gt;
		&lt;a href=&#34;https://www.zachstagers.co.uk/p/changing-a-data-factory-integration-runtime-registration-key/powershell_adf_integration_runtime_change.JPG&#34; data-size=&#34;1222x263&#34;&gt;&lt;img src=&#34;https://www.zachstagers.co.uk/p/changing-a-data-factory-integration-runtime-registration-key/powershell_adf_integration_runtime_change.JPG&#34;
				
				width=&#34;1222&#34;
				height=&#34;263&#34;
				loading=&#34;lazy&#34;
				alt=&#34;RegisterIntegrationRuntime.ps1 parameters and success message.&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;RegisterIntegrationRuntime.ps1 parameters and success message.&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;If you get either of the errors below, you may need to run &lt;code&gt;Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass&lt;/code&gt; in PowerShell before you can run the script:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;C:\Program Files\Microsoft Integration Runtime\5.0\PowerShellScript\RegisterIntegrationRuntime.ps1 cannot be loaded. The contents of file C:\Program Files\Microsoft Integration Runtime\5.0\PowerShellScript\RegisterIntegrationRuntime.ps1 might have been changed by an unauthorized user or process, because the hash of the file does not match the hash stored in the digital signature. The script cannot run on the specified system. For more information, run Get-Help about_Signing..&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Integration Runtime registration has failed with below : Cannot open DIAHostService service on computer&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;why-might-i-have-to-do-this&#34;&gt;Why might I have to do this?&lt;/h3&gt;
&lt;p&gt;To give a bit of context to those who&amp;rsquo;re simply hosting an IR inside a network for someone else to use, you might need to undertake this process if significant changes have occurred in the Azure environment using the IR. For example, they may be migrating to a different tenancy, subscription, or are setting up a different ADF IR implementation by linking all IR&amp;rsquo;s to a single ADF and sharing and managing them from there. It should be quite rare once in production that this needs to be done.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Connect Azure Databricks to a DevOps repo in a different tenancy</title>
        <link>https://www.zachstagers.co.uk/p/connect-azure-databricks-to-a-devops-repo-in-a-different-tenancy/</link>
        <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
        
        <guid>https://www.zachstagers.co.uk/p/connect-azure-databricks-to-a-devops-repo-in-a-different-tenancy/</guid>
        <description>&lt;img src="https://www.zachstagers.co.uk/p/connect-azure-databricks-to-a-devops-repo-in-a-different-tenancy/banner_databricks_devops_access_token.JPG" alt="Featured image of post Connect Azure Databricks to a DevOps repo in a different tenancy" /&gt;&lt;p&gt;If you&amp;rsquo;re working with a large multi-tenancy organisation it&amp;rsquo;s possible the subscription your Databricks resource sits in is a different tenancy to the Azure DevOps hosting your repositories. This blog explains how to connect Databricks to a DevOps repository in that scenario.&lt;/p&gt;
&lt;p&gt;When trying to connect to DevOps in a seperate tenancy, you&amp;rsquo;ll receive the message &lt;code&gt;Unable to parse credentials from Azure Active Directory account. Ensure Azure Devops account is connected to AAD.&lt;/code&gt; if you haven&amp;rsquo;t configured a Personal Access Token (PAT).&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Go to dev.azure.com and login to the DevOps organisation containing the repository you&amp;rsquo;re trying to connect Databricks to.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click the User Settings icon in the top right and go to Personal Access Tokens.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click + New Token&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fill in the Create a new personal access token form:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Give the token a sensible name, such as &amp;lsquo;Databricks Repo Token&amp;rsquo;&lt;/li&gt;
&lt;li&gt;Select the appropriate organisation&lt;/li&gt;
&lt;li&gt;Set the expiration as required&lt;/li&gt;
&lt;li&gt;The scope required &amp;lsquo;Full access&amp;rsquo;&lt;/li&gt;
&lt;li&gt;Press Create&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Copy the access token displayed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Go to your Databricks workspace.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click the workspace name in the top right and choose User Settings&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Go to the Git Intergration tab at the top of the page.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Change the drop down to Azure DevOps Services (personal access token)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Populate the Git provider username or email address with the email address you use to log in to the DevOps organisation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Paste the token copied in step 5 into the Token box and press save.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With that configured, you can now go back to Repos, select Add Repo, and clone the remote Git repo.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Azure Data Lake ACL Introduction</title>
        <link>https://www.zachstagers.co.uk/p/azure-data-lake-acl-introduction/</link>
        <pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate>
        
        <guid>https://www.zachstagers.co.uk/p/azure-data-lake-acl-introduction/</guid>
        <description>&lt;img src="https://www.zachstagers.co.uk/p/azure-data-lake-acl-introduction/banner_azure_datalake_acl.JPG" alt="Featured image of post Azure Data Lake ACL Introduction" /&gt;&lt;h3 id=&#34;introduction-to-acls&#34;&gt;Introduction to ACL&amp;rsquo;s&lt;/h3&gt;
&lt;p&gt;Access Control Lists (ACLs) offer low-level control of access to the folders within your Azure Data Lake, whilst Role-Based Access Control (RBAC) offers high-level control to the entire lake.&lt;/p&gt;
&lt;p&gt;&lt;figure style=&#34;flex-grow: 300; flex-basis: 720px&#34;&gt;
		&lt;a href=&#34;https://www.zachstagers.co.uk/p/azure-data-lake-acl-introduction/datalake_rbac_acl.JPG&#34; data-size=&#34;1000x333&#34;&gt;&lt;img src=&#34;https://www.zachstagers.co.uk/p/azure-data-lake-acl-introduction/datalake_rbac_acl.JPG&#34;
				
				width=&#34;1000&#34;
				height=&#34;333&#34;
				loading=&#34;lazy&#34;
				alt=&#34;RBAC vs ACL&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;RBAC vs ACL&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;The ACL permissions on offer are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Read&lt;/strong&gt; - Grants read access to files and folders (i.e. able to see the contents).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Write&lt;/strong&gt; - Grants write access to files and folders.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Execute&lt;/strong&gt; - Grants&amp;hellip; execute. What this actually means is the user is able to navigate through the folder. A user must have execute assigned to the entire hierarchy above the folder they have read and/or write access to. Whilst in the portal you can assign this to files, it doesn&amp;rsquo;t do anything.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As well as setting access explicitly to a folder, defaults can be set at any level to inherit permissions to &lt;em&gt;newly created&lt;/em&gt; sub folders and files, but note that these &lt;em&gt;do not&lt;/em&gt; apply to folders and files which already exist.&lt;/p&gt;
&lt;p&gt;Below is a visualisation of a simple folder structure with the minimum ACL&amp;rsquo;s required at each level to read data from &amp;ldquo;File 1&amp;rdquo;, whilst granting no access to &amp;ldquo;File 2&amp;rdquo;. Although this depicts the minimum permissions required to get to a single file, in reality you&amp;rsquo;d likely elevate the Read permission to &amp;ldquo;Sub Folder 1&amp;rdquo; as a default permission, therefore allowing it to inherit down to all files and folders listed underneath it.&lt;/p&gt;
&lt;p&gt;&lt;figure style=&#34;flex-grow: 175; flex-basis: 420px&#34;&gt;
		&lt;a href=&#34;https://www.zachstagers.co.uk/p/azure-data-lake-acl-introduction/datalake_acl_example.JPG&#34; data-size=&#34;905x516&#34;&gt;&lt;img src=&#34;https://www.zachstagers.co.uk/p/azure-data-lake-acl-introduction/datalake_acl_example.JPG&#34;
				
				width=&#34;905&#34;
				height=&#34;516&#34;
				loading=&#34;lazy&#34;
				alt=&#34;Data Lake ACL Example&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;Data Lake ACL Example&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;h3 id=&#34;testing-acls&#34;&gt;Testing ACL&amp;rsquo;s&lt;/h3&gt;
&lt;p&gt;There are a few things to be mindful of when testing with ACL&amp;rsquo;s:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Folder names are case sensitive.&lt;/li&gt;
&lt;li&gt;You need to connect to the lake using the Data Lake Storage end point - &lt;code&gt;https://&amp;lt;lakename&amp;gt;.dfs.core.windows.net/&amp;lt;container&amp;gt;/&amp;lt;folders&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;If testing via storage explorer, connect to the lake using the &amp;lsquo;ADLS Gen2 container or directory&amp;rsquo; option.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;the-mask-acl&#34;&gt;The Mask ACL&lt;/h3&gt;
&lt;p&gt;When managing ACL&amp;rsquo;s, there&amp;rsquo;s a button to Add principal, which allows you to select and add a user or group to assign ACL permissions to. There&amp;rsquo;s also a button to Add mask, and this defines an override of the effective permissions for named users and groups.&lt;/p&gt;
&lt;p&gt;In the below screenshot, I&amp;rsquo;ve added a mask and removed all permissions. Next to my Zach Stagers named user with Execute permissions you see a warning symbol. If you hover over the warning, it&amp;rsquo;ll say &amp;ldquo;The following access permissions are beyond the bounds of the mask: Execute&amp;rdquo;. This is just highlighting to you that although Zach Stagers has Execute, it won&amp;rsquo;t be effective because the Mask has disabled Execute permissions for all users.&lt;/p&gt;
&lt;p&gt;&lt;figure style=&#34;flex-grow: 185; flex-basis: 446px&#34;&gt;
		&lt;a href=&#34;https://www.zachstagers.co.uk/p/azure-data-lake-acl-introduction/manage_acl_mask.JPG&#34; data-size=&#34;688x370&#34;&gt;&lt;img src=&#34;https://www.zachstagers.co.uk/p/azure-data-lake-acl-introduction/manage_acl_mask.JPG&#34;
				
				width=&#34;688&#34;
				height=&#34;370&#34;
				loading=&#34;lazy&#34;
				alt=&#34;Manage ACL Screen with a mask applied&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;Manage ACL Screen with a mask applied&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;You might use this in scenarios where someone has put something sensitive or something they shouldn&amp;rsquo;t in a folder in the lake and you want an admin to be able to go in and remove it whilst restricting access for everyone else. This is a much easier method than removing all of your assigned ACL&amp;rsquo;s and then having to readd them.&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
