[{"content":"I blogged previously on my thoughts about what makes a good lakehouse serving layer - the functionality and features to consider when trying to identify the right serving tool for you.\nIn this post I want to go a step further and do a direct comparison of some of the more obvious Azure based lakehouse serving tools - Synapse Serverless, Databricks SQL, and Synapse Dedicated. I\u0026rsquo;ve decided to include Dedicated here, as although it defeats the purpose of the lakehouse paradigm, I\u0026rsquo;m still often asked whether it\u0026rsquo;s suitable.\nYou\u0026rsquo;ll notice I\u0026rsquo;ve put 2022 in the title of this blog. This is the type of thing that\u0026rsquo;ll have a changing landscape as these tools develop further, and new tools emerge, so I wanted to create that distinction now to support future editions of this post.\nFunctionality Comparison In my previous post, I called out 5 main things to look out for when choosing a tool; Concurrency, Security, Performance, Integration, and Cost. Below I\u0026rsquo;ve produced a comparison chart across these 5 metrics for the tools in scope for this blog.\n\r2022 Lakehouse Serving Tool Comparison Chart.\r\nFor reference, here\u0026rsquo;s a link to Synapse Serverless' Polaris Engine White Paper.\nThe comparison chart above condenses a lot of information into a single table, but of course there are other things to consider. Things like the maintenance involved with managing each of those tools, how arduous the set up and configuration is, and their unique abilities (or USP\u0026rsquo;s).\nSetup and Configuration By setup and configuration, I mean all of the work involved with getting to the point you can actually serve data from the tool - excluding your specific security setup\u0026hellip;\nSynapse Dedicated\nIn order to serve out of a dedicated SQL pool, you would first need to ingest the data into that pool. This could potentially include creating credentials, external file formats, external data source, external tables, and using PolyBase, or using the COPY command.\nOnce the data is into a dedicated pool, it may need merging from staging tables into serving tables using slowly changing techniques (SCD).\nThat\u0026rsquo;s quite a lot of configuration, plus you have to pay an additional ~Â£18 per TB of data stored inside the dedicated pool each month.\nSynapse Serverless\nSimilar to Dedicated, but a little simpler. Here we need to provide a method of reading from the lake which may again include creating credentials, external file formats, and external data sources - plus views over those entities from which to serve, but there\u0026rsquo;s no PolyBase or COPY or data storage.\nI\u0026rsquo;ve blogged previously about Azure Synapse Serverless Lake Access Patterns, so I won\u0026rsquo;t go into any more detail on that here.\nDatabricks SQL\nPerhaps the easiest of the lot. Assuming that you\u0026rsquo;re using Databricks as your processing engine, you probably already have your tables saved within the Hive metastore, which makes them available for querying in Databricks SQL.\nIf you\u0026rsquo;re not using Databricks for your processing, then you\u0026rsquo;d need to mount your lake, create a database, and create your tables specifying the lake location to be read from.\nMaintenance Synapse Dedicated\nManaging a Dedicated pool can be quite involved. You need to think about the distribution method used for each table to avoid skew, whether table replication is appropriate, and the more traditional maintenance that comes with indexing.\nSynapse Serverless\nAside from ensuring your lake is in order, there really isn\u0026rsquo;t any maintenance involved with Serverless - that\u0026rsquo;s the beauty of Polaris. If you\u0026rsquo;re using the Delta format (you should be) then you\u0026rsquo;ll want to apply vacuuming and optimising occasionally to keep things tip top.\nDatabricks SQL\nWith Databricks SQL you have an endpoint/clusters to monitor and manage, which comes with some overhead. On top of that, you should again ensure your Delta files are in good shape by running a vacuum and optimise every now and then.\nUnique Selling Points Synapse Dedicated\nIt doesn\u0026rsquo;t really have any unique serving capabilities when compared to Synapse Serverless and Databricks SQL.\nSynapse Serverless\nThe beauty of Serverless is the Polaris engine. There\u0026rsquo;s absolutely nothing to manage - no cluster to spin up, or sizing. On the other hand, this means it\u0026rsquo;s a little black boxy, but there\u0026rsquo;s plenty of guidance out there about how to optimise serverless queries (hint: use Delta and specify a WITH clause with narrow data types!).\nDatabricks SQL\nDatabricks SQL offers the in-built ability to create visualisations, dashboards, and alerts. Meaning, if it caters to all your visualisation needs, and you go all in on Databricks, you may not even need PowerBI licencing! Suddenly the price tag for a Databricks SQL cluster really doesn\u0026rsquo;t look too bad\u0026hellip;\nConclusion Hopefully this post has helped you decide on how you want to serve your lake. On the surface it may appear that Serverless is the outright winner here, but Databricks SQL is a fantastic option too. If you\u0026rsquo;re only using Synapse for one of it\u0026rsquo;s many capabilities (in this case Serverless), the fact you could eliminate it from your architecture all together and therefore not have another resource/workspace to manage is a really big deal.\nIt feels as though I\u0026rsquo;ve been pretty harsh on ol' Synapse Dedicated SQL Pools today\u0026hellip; but in the age of lakes, it is not king, and I hope to have painted a clear and obvious picture of the reasons why it isn\u0026rsquo;t suitable for serving a lake-based analytical model.\n","date":"2022-05-15T00:00:00Z","image":"https://www.zachstagers.co.uk/p/lakehouse-serving-options-2022/banner_lakehouse_serving_options_2022.jpg","permalink":"https://www.zachstagers.co.uk/p/lakehouse-serving-options-2022/","title":"Lakehouse Serving Options - 2022"},{"content":"This post is a bit of an exploratory one, collecting some thoughts about features which combine to make a good data lakehouse serving layer, and the things to consider when choosing a resource to serve your data.\nI do not intend for this post to act as a guide for choosing the right serving layer, but instead it\u0026rsquo;s an introduction to the subject and some things to watch out for as you identify which tool would be best for you.\nWhat is a serving layer? Simply put, a serving layer is a method of presenting data to users. Or put differently, it\u0026rsquo;s a tool through which users can access data.\nBut when you break that simple statement down, there\u0026rsquo;s a lot to think about! What data is being presented back, and who is it being presented to? How do users want to connect to the serving layer (assuming they have a choice)? And when they\u0026rsquo;ve connected, how do we ensure they only see the data they\u0026rsquo;re supposed to see? How much data is there? How many users are there? What timezones do they operate in?\nTaking the answers to these questions, we can build a picture of our requirements, and therefore the features we need to think about and therefore what makes a serving layer good.\nWhy do you need a serving layer? A big selling point of the data lakehouse is the lakes ability to service a broad range of users, directly from the lake. But all the lake does is store data - it has no compute ability with which to query that data.\nRegardless of whether you have analysts exploring the lake or you have a centrally-controlled and developed enterprise model, you need a compute resource to query that lake data.\nWhat features do we look for in a serving area? 1. Concurrency\nIf we know we have 100\u0026rsquo;s or even 1,000\u0026rsquo;s of users who\u0026rsquo;ll be querying our data model every day, it\u0026rsquo;s no use copying our data into a Synapse Dedicated pool. Not only would that be an expensive option, but dedicated pool\u0026rsquo;s maximum concurrency limit of 128 would quickly cause delays and performance issues. It would also defeat the purpose of the lakehouse paradigm by duplicating your data to somewhere outside of the lake.\nWe need to ensure the tool can handle the number of users and queries, remembering that each visual on a dashboard equates to a query, this can add up fast.\n2. Security\nRow Level Security (RLS) has long been method for enhancing data security at the serving layer, and is often an important feature to look out for in the majority of projects.\nOther things to consider here is how well it integrates with your identity management system, whether that be Azure Active Directory (AAD) or something like Okta.\n3. Performance\nPerformance is one of those things that can go completely unnoticed when something is fast, but can be the bane of everyone\u0026rsquo;s life when it\u0026rsquo;s slow!\nSome things to consider in this area are whether the first users to query each day will have to wait for a resource or cluster to start, or in a serverless scenario, how long does it generally take for a query to even begin running.\nOn the other hand, if we\u0026rsquo;re importing data into a PowerBI model, how long does that process take? This is especially important if we want to refresh our models throughout the day.\n4. Integration\nThere\u0026rsquo;s two sides to the serving layer to consider here. The integration between the lake and the serving layer, and the integration between the serving layer and user reporting tools like PowerBI.\nOn the lake to serving layer integration, if we\u0026rsquo;re using a direct-query model, it absolutely must understand how to read a parquet file or delta table to be a suitable option for serving a data lakehouse. Without this, querying any datasets on the larger side would likely be unacceptably slow for users.\nWhereas on the other side, does it play nicely with the reporting tools users will be connecting with such as PowerBI, Tableau, Looker, \u0026hellip;or Excel.\n5. Cost\nOK, cost isn\u0026rsquo;t really a feature, but it\u0026rsquo;s almost always one of the biggest factors in choosing any component of a data platform.\nThings to consider when you\u0026rsquo;re on a tight budget is whether the resource needs a cluster up and running, and if so what the uptime scheduling for that looks like and how big the cluster needs to be to support the number of users you\u0026rsquo;re expecting.\nWhat options are there? Gone are the days of Analysis Services' dominance in the serving arena! There are a host of tools available which offer lake based serving capabilities.\n Azure Synapse Databricks Azure Analysis Services PowerBI (Import Mode Only)  These are obviously very Azure / Microsoft focused, which is the space I tend to operate within, but of course there are plenty of other vendors which can read and serve lake data too.\nConclusion As I\u0026rsquo;ve written these thoughts and considerations out, it\u0026rsquo;s dawned on me just how complex and nuanced a topic this is!\nIn a future post I\u0026rsquo;d like to explore the serving options further, their strengths and weaknesses in the areas I\u0026rsquo;ve mentioned in this post, and the circumstances under which each may be the most appropriate choice.\nI\u0026rsquo;d be interested to hear other peoples thoughts on this subject.\n","date":"2022-03-30T00:00:00Z","image":"https://www.zachstagers.co.uk/p/what-makes-a-good-lakehouse-serving-layer/banner_what_makes_a_good_lakehouse_serving_layer.jpg","permalink":"https://www.zachstagers.co.uk/p/what-makes-a-good-lakehouse-serving-layer/","title":"What Makes a Good Lakehouse Serving Layer?"},{"content":"I\u0026rsquo;ve been working with a client to build a large Data Lakehouse platform predominantly using Databricks as a processing engine and Synapse Serverless to serve the data out in a couple different ways.\nThis was the first time I\u0026rsquo;d properly used Synapse Serverless in anger, and to say I found the documentation around lake access confusing would be an understatement! Everything I needed to understand was spread over a few different articles, which I\u0026rsquo;ve linked to at the bottom of this post.\nThe crux of it is that there\u0026rsquo;s several ways to configure access to your lake data from Synapse Serverless, and in this blog I\u0026rsquo;d like to step through the ways we\u0026rsquo;ve configured it, and what those configurations are good for. This post does not speak to network security, although I have provided a link to the Azure Synapse Network Security white paper at the end.\nActive Directory (AD) Passthrough (a.k.a User Identity) This is the default method of accessing data. If you simply open up your Synapse workspace and query your lake data using OpenRowset queries without creating and specifying a data source, your credentials will be passed down to the data lake layer to be authenticated.\nThis means that in order for authentication to succeed, you need to have configured either RBAC or ACL\u0026rsquo;s for your user account;\n RBAC (Role Based Access Control) - Course grain access control. Whatever RBAC role is assigned will apply to all files and folders in the lake. The role applied to permit access via Serverless must be one of the \u0026lsquo;Storage Blob Data ***\u0026rsquo; roles. ACL (Access Control Lists) - Fine grain access control. Grants access to specific files and folders. For a deeper look at ACL\u0026rsquo;s, see a previous blog of mine here: Azure Data Lake ACL Introduction.  This option is great in two different ways, depending on which security protocol you use:\n For allowing analysts free reign into the lake (RBAC). To have meticulous control over what people have access to (ACL).  Synapse Managed Service Identity (MSI) - Database Scoped If you don\u0026rsquo;t want to configure ACL\u0026rsquo;s or RBAC for your users, you can grant access via the Synapse managed service identity (MSI) using a database scoped credential. Managed identities are essentially a credential that is created alongside the resource which can be treated like any other active directory account - they typically have the same name as the resource they belong to. The MSI will still need to be granted lake level access (i.e. via RBAC or ACL, per the above section).\nTo make use of the MSI, some set up is required. As we\u0026rsquo;re creating a database scoped, you\u0026rsquo;ll need to create a serverless database, then from within that database you need to create a master key to protect our credential.\nNext we create a database scoped credential specifying that the credential is based on the managed service identity.\nThe final step is to create an external data source. This specifies our lake location via the data lake storage end point URL, and the credential to use to connect to that end point (in this case this will be our MSI credential). The location specified is the root of where access will be granted, meaning users accessing via this data source will have access to everything beneath the location specified.\nPutting all of this together, we end up with this script to enable lake access:\nCREATE DATABASE SomeDatabase\rUSE SomeDatabase\rCREATE MASTER KEY ENCRYPTION BY PASSWORD='\u0026lt;SuperStrongPassw0rd\u0026gt;'\rCREATE DATABASE SCOPED CREDENTIAL ManagedIdentityCredential\rWITH IDENTITY = 'MANAGED SERVICE IDENTITY'\rCREATE EXTERNAL DATA SOURCE DataLakeContainer\rWITH (\rLOCATION = 'https://\u0026lt;storageAccount\u0026gt;.dfs.core.windows.net/container/',\rCREDENTIAL = ManagedIdentityCredential\r);\rTo use the data source, we adapt our queries to include a data source parameter in our OpenRowset. Notice now that the BULK parameter, which would by default include the entire lake path, now only needs the path down from the location specified in our data source.\nSELECT\rTOP 100 *\rFROM\rOPENROWSET(\rBULK '/folder/deltaTable',\rFORMAT = 'DELTA',\rDATA_SOURCE = 'DataLakeContainer'\r) AS [result]\rThis option is again good in multiple ways:\n For allowing analysts free reign into the lake from a certain access point and below. For developing a standard set of views which expose data to PowerBI.  Wrap Up Using both of these methods together gives us a powerful blend of control and access. We\u0026rsquo;re able to serve a standardised and trusted enterprise data model to PowerBI using the MSI method, and we\u0026rsquo;re able to control access for analysts to splash around in the lake building their own models and metrics.\nThis isn\u0026rsquo;t an exhaustive list of access methods - you could also use shared access signatures or service principals.\nCheck out some of the security documentation for yourself:\n Serverless SQL pool in Azure Synapse Analytics Control storage account access for serverless SQL pool in Azure Synapse Analytics Self-help for serverless SQL pool Azure Synapse Analytics security white paper: Network Security  ","date":"2022-03-23T00:00:00Z","image":"https://www.zachstagers.co.uk/p/azure-synapse-serverless-lake-access-patterns/banner_azure_synapse_serverless_lake_access_patterns.jpg","permalink":"https://www.zachstagers.co.uk/p/azure-synapse-serverless-lake-access-patterns/","title":"Azure Synapse Serverless Lake Access Patterns"},{"content":"This years arcade-themed SQLBits is taking place between March 8th and 12th. I\u0026rsquo;ve been to SQLBits a number of times, it\u0026rsquo;s always great fun and I come away having learned a ton!\nHowever, this year is a little different from me, as I\u0026rsquo;ll be presenting for the first time! I\u0026rsquo;m looking forward to it a lot, the last time I presented at a community event was on Azure Notebooks back in July 2018 (as seen in the banner image)!\nPre-Con: A Data Engineers Guide to Synapse Analytics Alongside Simon Whiteley and Stijn Wynants, I\u0026rsquo;ll be presenting a full day of Synapse training on Tuesday the 8th of March!\nIt\u0026rsquo;ll be an action packed day covering:\n Fundamentals of a lake based data platform Patterns for ingesting data and orchestrating your data platform execution How SQL Serverless Pools work and patterns for optimising performance and cost Using Spark Pools to write dynamic workflows in Python Utilizing Data Explorer Pools for deep exploration of logs, time series and other fast-moving unstructured data sources Integrating Synapse directly with tools such as Azure Purview and CosmosDB  You can learn more from our sessionize page, or by watching our short promotional clip\n\r\rSession: Synapse Data Flows - Will Citizen ETL Replace the Data Engineer? As well as the pre-con, I\u0026rsquo;ll be presenting my own much shorter 20-minute session on Synapse Data Flows on Saturday the 12th of March, taking a look at how they measure up when compared to bespoke-built data platform. What\u0026rsquo;s the pipeline reusibility like? How well can you clean and standardize your data? What are they good at, and what are they bad at?\nYou can learn more from my sessionize page.\nOther sessions to look out for Advancing Analytics will be at SQLBits in force. All together, we\u0026rsquo;re presenting 3 full day pre-conference training days and 11 general sessions! Find out what we\u0026rsquo;ll be presenting about and when via the Advancing Analytics blog.\nThe full conference agenda can be found here.\n","date":"2022-02-02T00:00:00Z","image":"https://www.zachstagers.co.uk/p/sqlbits-2022/banner_sqlbits_2022.jpg","permalink":"https://www.zachstagers.co.uk/p/sqlbits-2022/","title":"SQLBits 2022"},{"content":"Introduction When you\u0026rsquo;ve installed an integration runtime (IR) and connected it to an Azure Data Factory (ADF), it isn\u0026rsquo;t exactly obvious how you change which Data Factory that IR is connected to. This short blog talks you through the process of updating an IR\u0026rsquo;s connection.\nSteps to change the IR\u0026rsquo;s Key Firstly, confirm the IR is in fact already registered to a Data Factory. Open the Integration Runtime Configuration Manager and you should see a green tick and \u0026lsquo;Self-hosted node is connected to the cloud service\u0026rsquo; with details of the ADF resource it\u0026rsquo;s connected to. If you don\u0026rsquo;t see this, and instead see a text box with \u0026lsquo;Register Integration Runtime (Self-hosted)\u0026rsquo; above it then your IR isn\u0026rsquo;t registered and you can skip all of the steps below and just paste the key into the registration box and hit go.\nOn to how to change which ADF resource your IR is registered with\u0026hellip;\nWhen you install an IR, a PowerShell script is placed alongside the installation which can be used for this task. Note that your folder path may not be identical to mine, as your IR may not have been installed in the default location, or you may have a different IR version.\n  From the machine hosting the IR, open the Windows PowerShell ISE as Administrator (right click, Run as admin\u0026hellip;)\n  Within the ISE click File, Open, and navigating to: C:\\Program Files\\Microsoft Integration Runtime\\5.0\\PowerShellScript\n  Open the RegisterIntegrationRuntime.ps1 PowerShell script.\n  Populate the parameters:\n $gatewayKey is the registration key of the new ADF resource you want to register the IR with. Remove the $throw so it\u0026rsquo;s just a string of the key. $IsRegisteringOnRemoteMachine can be left as it\u0026rsquo;s default, false, as in step 1 we\u0026rsquo;re running this script from the machine hosting the IR. $NodeName is the name that\u0026rsquo;ll appear in ADF when it\u0026rsquo;s registered. This is typically populated with the machine name hosting the IR.    Your script should look something like the screenshot below. Press run, it may take a minute or so, but once successful you\u0026rsquo;ll see the message \u0026lsquo;Integration Runtime registration is successful!\u0026rsquo;\n  Re-open the Integration Runtime Configuration Manager and confirm it is now registered with the new IR.\n  \rRegisterIntegrationRuntime.ps1 parameters and success message.\r\nIf you get either of the errors below, you may need to run Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass in PowerShell before you can run the script:\n C:\\Program Files\\Microsoft Integration Runtime\\5.0\\PowerShellScript\\RegisterIntegrationRuntime.ps1 cannot be loaded. The contents of file C:\\Program Files\\Microsoft Integration Runtime\\5.0\\PowerShellScript\\RegisterIntegrationRuntime.ps1 might have been changed by an unauthorized user or process, because the hash of the file does not match the hash stored in the digital signature. The script cannot run on the specified system. For more information, run Get-Help about_Signing.. Integration Runtime registration has failed with below : Cannot open DIAHostService service on computer  Why might I have to do this? To give a bit of context to those who\u0026rsquo;re simply hosting an IR inside a network for someone else to use, you might need to undertake this process if significant changes have occurred in the Azure environment using the IR. For example, they may be migrating to a different tenancy, subscription, or are setting up a different ADF IR implementation by linking all IR\u0026rsquo;s to a single ADF and sharing and managing them from there. It should be quite rare once in production that this needs to be done.\n","date":"2021-12-14T00:00:00Z","image":"https://www.zachstagers.co.uk/p/changing-a-data-factory-integration-runtime-registration-key/banner_change_adf_ir.JPG","permalink":"https://www.zachstagers.co.uk/p/changing-a-data-factory-integration-runtime-registration-key/","title":"Changing a Data Factory Integration Runtime Registration Key"},{"content":"If you\u0026rsquo;re working with a large multi-tenancy organisation it\u0026rsquo;s possible the subscription your Databricks resource sits in is a different tenancy to the Azure DevOps hosting your repositories. This blog explains how to connect Databricks to a DevOps repository in that scenario.\nWhen trying to connect to DevOps in a seperate tenancy, you\u0026rsquo;ll receive the message Unable to parse credentials from Azure Active Directory account. Ensure Azure Devops account is connected to AAD. if you haven\u0026rsquo;t configured a Personal Access Token (PAT).\n  Go to dev.azure.com and login to the DevOps organisation containing the repository you\u0026rsquo;re trying to connect Databricks to.\n  Click the User Settings icon in the top right and go to Personal Access Tokens.\n  Click + New Token\n  Fill in the Create a new personal access token form:\n Give the token a sensible name, such as \u0026lsquo;Databricks Repo Token\u0026rsquo; Select the appropriate organisation Set the expiration as required The scope required \u0026lsquo;Full access\u0026rsquo; Press Create    Copy the access token displayed.\n  Go to your Databricks workspace.\n  Click the workspace name in the top right and choose User Settings\n  Go to the Git Intergration tab at the top of the page.\n  Change the drop down to Azure DevOps Services (personal access token)\n  Populate the Git provider username or email address with the email address you use to log in to the DevOps organisation.\n  Paste the token copied in step 5 into the Token box and press save.\n  With that configured, you can now go back to Repos, select Add Repo, and clone the remote Git repo.\n","date":"2021-09-01T00:00:00Z","image":"https://www.zachstagers.co.uk/p/connect-azure-databricks-to-a-devops-repo-in-a-different-tenancy/banner_databricks_devops_access_token.JPG","permalink":"https://www.zachstagers.co.uk/p/connect-azure-databricks-to-a-devops-repo-in-a-different-tenancy/","title":"Connect Azure Databricks to a DevOps repo in a different tenancy"},{"content":"Introduction to ACL\u0026rsquo;s Access Control Lists (ACLs) offer low-level control of access to the folders within your Azure Data Lake, whilst Role-Based Access Control (RBAC) offers high-level control to the entire lake.\n\rRBAC vs ACL\r\nThe ACL permissions on offer are:\n Read - Grants read access to files and folders (i.e. able to see the contents). Write - Grants write access to files and folders. Execute - Grants\u0026hellip; execute. What this actually means is the user is able to navigate through the folder. A user must have execute assigned to the entire hierarchy above the folder they have read and/or write access to. Whilst in the portal you can assign this to files, it doesn\u0026rsquo;t do anything.  As well as setting access explicitly to a folder, defaults can be set at any level to inherit permissions to newly created sub folders and files, but note that these do not apply to folders and files which already exist.\nBelow is a visualisation of a simple folder structure with the minimum ACL\u0026rsquo;s required at each level to read data from \u0026ldquo;File 1\u0026rdquo;, whilst granting no access to \u0026ldquo;File 2\u0026rdquo;. Although this depicts the minimum permissions required to get to a single file, in reality you\u0026rsquo;d likely elevate the Read permission to \u0026ldquo;Sub Folder 1\u0026rdquo; as a default permission, therefore allowing it to inherit down to all files and folders listed underneath it.\n\rData Lake ACL Example\r\nTesting ACL\u0026rsquo;s There are a few things to be mindful of when testing with ACL\u0026rsquo;s:\n Folder names are case sensitive. You need to connect to the lake using the Data Lake Storage end point - https://\u0026lt;lakename\u0026gt;.dfs.core.windows.net/\u0026lt;container\u0026gt;/\u0026lt;folders\u0026gt; If testing via storage explorer, connect to the lake using the \u0026lsquo;ADLS Gen2 container or directory\u0026rsquo; option.  The Mask ACL When managing ACL\u0026rsquo;s, there\u0026rsquo;s a button to Add principal, which allows you to select and add a user or group to assign ACL permissions to. There\u0026rsquo;s also a button to Add mask, and this defines an override of the effective permissions for named users and groups.\nIn the below screenshot, I\u0026rsquo;ve added a mask and removed all permissions. Next to my Zach Stagers named user with Execute permissions you see a warning symbol. If you hover over the warning, it\u0026rsquo;ll say \u0026ldquo;The following access permissions are beyond the bounds of the mask: Execute\u0026rdquo;. This is just highlighting to you that although Zach Stagers has Execute, it won\u0026rsquo;t be effective because the Mask has disabled Execute permissions for all users.\n\rManage ACL Screen with a mask applied\r\nYou might use this in scenarios where someone has put something sensitive or something they shouldn\u0026rsquo;t in a folder in the lake and you want an admin to be able to go in and remove it whilst restricting access for everyone else. This is a much easier method than removing all of your assigned ACL\u0026rsquo;s and then having to readd them.\n","date":"2021-08-13T00:00:00Z","image":"https://www.zachstagers.co.uk/p/azure-data-lake-acl-introduction/banner_azure_datalake_acl.JPG","permalink":"https://www.zachstagers.co.uk/p/azure-data-lake-acl-introduction/","title":"Azure Data Lake ACL Introduction"}]