[{"content":"This years arcade-themed SQLBits is taking place between March 8th and 12th. I\u0026rsquo;ve been to SQLBits a number of times, it\u0026rsquo;s always great fun and I come away having learned a ton!\nHowever, this year is a little different from me, as I\u0026rsquo;ll be presenting for the first time! I\u0026rsquo;m looking forward to it a lot, the last time I presented at a community event was on Azure Notebooks back in July 2018 (as seen in the banner image)!\nPre-Con: A Data Engineers Guide to Synapse Analytics Alongside Simon Whiteley and Stijn Wynants, I\u0026rsquo;ll be presenting a full day of Synapse training on Tuesday the 8th of March!\nIt\u0026rsquo;ll be an action packed day covering:\n Fundamentals of a lake based data platform Patterns for ingesting data and orchestrating your data platform execution How SQL Serverless Pools work and patterns for optimising performance and cost Using Spark Pools to write dynamic workflows in Python Utilizing Data Explorer Pools for deep exploration of logs, time series and other fast-moving unstructured data sources Integrating Synapse directly with tools such as Azure Purview and CosmosDB  You can learn more from our sessionize page, or by watching our short promotional clip\n\r\rSession: Synapse Data Flows - Will Citizen ETL Replace the Data Engineer? As well as the pre-con, I\u0026rsquo;ll be presenting my own much shorter 20-minute session on Synapse Data Flows on Saturday the 12th of March, taking a look at how they measure up when compared to bespoke-built data platform. What\u0026rsquo;s the pipeline reusibility like? How well can you clean and standardize your data? What are they good at, and what are they bad at?\nYou can learn more from my sessionize page.\nOther sessions to look out for Advancing Analytics will be at SQLBits in force. All together, we\u0026rsquo;re presenting 3 full day pre-conference training days and 11 general sessions! Find out what we\u0026rsquo;ll be presenting about and when via the Advancing Analytics blog.\nThe full conference agenda can be found here.\n","date":"2022-02-02T00:00:00Z","image":"https://www.zachstagers.co.uk/p/sqlbits-2022/banner_sqlbits_2022.jpg","permalink":"https://www.zachstagers.co.uk/p/sqlbits-2022/","title":"SQLBits 2022"},{"content":"Introduction When you\u0026rsquo;ve installed an integration runtime (IR) and connected it to an Azure Data Factory (ADF), it isn\u0026rsquo;t exactly obvious how you change which Data Factory that IR is connected to. This short blog talks you through the process of updating an IR\u0026rsquo;s connection.\nSteps to change the IR\u0026rsquo;s Key Firstly, confirm the IR is in fact already registered to a Data Factory. Open the Integration Runtime Configuration Manager and you should see a green tick and \u0026lsquo;Self-hosted node is connected to the cloud service\u0026rsquo; with details of the ADF resource it\u0026rsquo;s connected to. If you don\u0026rsquo;t see this, and instead see a text box with \u0026lsquo;Register Integration Runtime (Self-hosted)\u0026rsquo; above it then your IR isn\u0026rsquo;t registered and you can skip all of the steps below and just paste the key into the registration box and hit go.\nOn to how to change which ADF resource your IR is registered with\u0026hellip;\nWhen you install an IR, a PowerShell script is placed alongside the installation which can be used for this task. Note that your folder path may not be identical to mine, as your IR may not have been installed in the default location, or you may have a different IR version.\n  From the machine hosting the IR, open the Windows PowerShell ISE as Administrator (right click, Run as admin\u0026hellip;)\n  Within the ISE click File, Open, and navigating to: C:\\Program Files\\Microsoft Integration Runtime\\5.0\\PowerShellScript\n  Open the RegisterIntegrationRuntime.ps1 PowerShell script.\n  Populate the parameters:\n $gatewayKey is the registration key of the new ADF resource you want to register the IR with. Remove the $throw so it\u0026rsquo;s just a string of the key. $IsRegisteringOnRemoteMachine can be left as it\u0026rsquo;s default, false, as in step 1 we\u0026rsquo;re running this script from the machine hosting the IR. $NodeName is the name that\u0026rsquo;ll appear in ADF when it\u0026rsquo;s registered. This is typically populated with the machine name hosting the IR.    Your script should look something like the screenshot below. Press run, it may take a minute or so, but once successful you\u0026rsquo;ll see the message \u0026lsquo;Integration Runtime registration is successful!\u0026rsquo;\n  Re-open the Integration Runtime Configuration Manager and confirm it is now registered with the new IR.\n  \rRegisterIntegrationRuntime.ps1 parameters and success message.\r\nIf you get either of the errors below, you may need to run Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass in PowerShell before you can run the script:\n C:\\Program Files\\Microsoft Integration Runtime\\5.0\\PowerShellScript\\RegisterIntegrationRuntime.ps1 cannot be loaded. The contents of file C:\\Program Files\\Microsoft Integration Runtime\\5.0\\PowerShellScript\\RegisterIntegrationRuntime.ps1 might have been changed by an unauthorized user or process, because the hash of the file does not match the hash stored in the digital signature. The script cannot run on the specified system. For more information, run Get-Help about_Signing.. Integration Runtime registration has failed with below : Cannot open DIAHostService service on computer  Why might I have to do this? To give a bit of context to those who\u0026rsquo;re simply hosting an IR inside a network for someone else to use, you might need to undertake this process if significant changes have occurred in the Azure environment using the IR. For example, they may be migrating to a different tenancy, subscription, or are setting up a different ADF IR implementation by linking all IR\u0026rsquo;s to a single ADF and sharing and managing them from there. It should be quite rare once in production that this needs to be done.\n","date":"2021-12-14T00:00:00Z","image":"https://www.zachstagers.co.uk/p/changing-a-data-factory-integration-runtime-registration-key/banner_change_adf_ir.JPG","permalink":"https://www.zachstagers.co.uk/p/changing-a-data-factory-integration-runtime-registration-key/","title":"Changing a Data Factory Integration Runtime Registration Key"},{"content":"If you\u0026rsquo;re working with a large multi-tenancy organisation it\u0026rsquo;s possible the subscription your Databricks resource sits in is a different tenancy to the Azure DevOps hosting your repositories. This blog explains how to connect Databricks to a DevOps repository in that scenario.\nWhen trying to connect to DevOps in a seperate tenancy, you\u0026rsquo;ll receive the message Unable to parse credentials from Azure Active Directory account. Ensure Azure Devops account is connected to AAD. if you haven\u0026rsquo;t configured a Personal Access Token (PAT).\n  Go to dev.azure.com and login to the DevOps organisation containing the repository you\u0026rsquo;re trying to connect Databricks to.\n  Click the User Settings icon in the top right and go to Personal Access Tokens.\n  Click + New Token\n  Fill in the Create a new personal access token form:\n Give the token a sensible name, such as \u0026lsquo;Databricks Repo Token\u0026rsquo; Select the appropriate organisation Set the expiration as required The scope required \u0026lsquo;Full access\u0026rsquo; Press Create    Copy the access token displayed.\n  Go to your Databricks workspace.\n  Click the workspace name in the top right and choose User Settings\n  Go to the Git Intergration tab at the top of the page.\n  Change the drop down to Azure DevOps Services (personal access token)\n  Populate the Git provider username or email address with the email address you use to log in to the DevOps organisation.\n  Paste the token copied in step 5 into the Token box and press save.\n  With that configured, you can now go back to Repos, select Add Repo, and clone the remote Git repo.\n","date":"2021-09-01T00:00:00Z","image":"https://www.zachstagers.co.uk/p/connect-azure-databricks-to-a-devops-repo-in-a-different-tenancy/banner_databricks_devops_access_token.JPG","permalink":"https://www.zachstagers.co.uk/p/connect-azure-databricks-to-a-devops-repo-in-a-different-tenancy/","title":"Connect Azure Databricks to a DevOps repo in a different tenancy"},{"content":"Introduction to ACL\u0026rsquo;s Access Control Lists (ACLs) offer low-level control of access to the folders within your Azure Data Lake, whilst Role-Based Access Control (RBAC) offers high-level control to the entire lake.\n\rRBAC vs ACL\r\nThe ACL permissions on offer are:\n Read - Grants read access to files and folders (i.e. able to see the contents). Write - Grants write access to files and folders. Execute - Grants\u0026hellip; execute. What this actually means is the user is able to navigate through the folder. A user must have execute assigned to the entire hierarchy above the folder they have read and/or write access to. Whilst in the portal you can assign this to files, it doesn\u0026rsquo;t do anything.  As well as setting access explicitly to a folder, defaults can be set at any level to inherit permissions to newly created sub folders and files, but note that these do not apply to folders and files which already exist.\nBelow is a visualisation of a simple folder structure with the minimum ACL\u0026rsquo;s required at each level to read data from \u0026ldquo;File 1\u0026rdquo;, whilst granting no access to \u0026ldquo;File 2\u0026rdquo;. Although this depicts the minimum permissions required to get to a single file, in reality you\u0026rsquo;d likely elevate the Read permission to \u0026ldquo;Sub Folder 1\u0026rdquo; as a default permission, therefore allowing it to inherit down to all files and folders listed underneath it.\n\rData Lake ACL Example\r\nTesting ACL\u0026rsquo;s There are a few things to be mindful of when testing with ACL\u0026rsquo;s:\n Folder names are case sensitive. You need to connect to the lake using the Data Lake Storage end point - https://\u0026lt;lakename\u0026gt;.dfs.core.windows.net/\u0026lt;container\u0026gt;/\u0026lt;folders\u0026gt; If testing via storage explorer, connect to the lake using the \u0026lsquo;ADLS Gen2 container or directory\u0026rsquo; option.  The Mask ACL When managing ACL\u0026rsquo;s, there\u0026rsquo;s a button to Add principal, which allows you to select and add a user or group to assign ACL permissions to. There\u0026rsquo;s also a button to Add mask, and this defines an override of the effective permissions for named users and groups.\nIn the below screenshot, I\u0026rsquo;ve added a mask and removed all permissions. Next to my Zach Stagers named user with Execute permissions you see a warning symbol. If you hover over the warning, it\u0026rsquo;ll say \u0026ldquo;The following access permissions are beyond the bounds of the mask: Execute\u0026rdquo;. This is just highlighting to you that although Zach Stagers has Execute, it won\u0026rsquo;t be effective because the Mask has disabled Execute permissions for all users.\n\rManage ACL Screen with a mask applied\r\nYou might use this in scenarios where someone has put something sensitive or something they shouldn\u0026rsquo;t in a folder in the lake and you want an admin to be able to go in and remove it whilst restricting access for everyone else. This is a much easier method than removing all of your assigned ACL\u0026rsquo;s and then having to readd them.\n","date":"2021-08-13T00:00:00Z","image":"https://www.zachstagers.co.uk/p/azure-data-lake-acl-introduction/banner_azure_datalake_acl.JPG","permalink":"https://www.zachstagers.co.uk/p/azure-data-lake-acl-introduction/","title":"Azure Data Lake ACL Introduction"}]